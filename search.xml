<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>『3Blue1Brown系列』GPT是什么</title>
      <link href="/2024/05/21/pytorch2/"/>
      <url>/2024/05/21/pytorch2/</url>
      
        <content type="html"><![CDATA[<h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>GPT全称是Generative Pre-trained Transformer。<br>GPT生成的每一个词也来自于概率的预测，预测出当前词汇后再结合前面的所有词汇去预测下一个单词，即反复的预测、抽样、追加。<br>首先来总览一下数据是如何在各个模块之间流动的。</p><ul><li>输入内容会被切分成许多小片段，称为token。在文本中，token往往是单词。在图像和音频中，token一般是小块图像和声音片段。每个token对应一个向量（即一组数字），旨在设法编码该片段的意义，如下图1所示。<br><img src="/img/gpt11.jpg" alt=""></li><li>这些向量随后经过『Attention模块』的处理，使得向量之间可以互相交流，通过相互传递信息来更改自己的值。<br><img src="/img/gpt22.jpg" alt=""></li><li>之后这些向量会经过另一种处理，称为MLP『Multi-layer perceptron』或『feed-forward』。此时，向量不再相互交流，而是并行经过同一处理。</li><li>最后的统一目标是能将整段文字的所有关键含义以某种方式融入到序列的最后一个向量。然后对这个向量进行某种操作，得出所有可能token的概率分布<h1 id="背景知识"><a href="#背景知识" class="headerlink" title="背景知识"></a>背景知识</h1><h1 id="Embedding"><a href="#Embedding" class="headerlink" title="Embedding"></a>Embedding</h1><h1 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h1><h1 id="MLPs"><a href="#MLPs" class="headerlink" title="MLPs"></a>MLPs</h1><h1 id="Unembedding"><a href="#Unembedding" class="headerlink" title="Unembedding"></a>Unembedding</h1></li></ul>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> Transformer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>『二分法查找』算法小结</title>
      <link href="/2024/05/20/leetcode/"/>
      <url>/2024/05/20/leetcode/</url>
      
        <content type="html"><![CDATA[<h1 id="二分查找模板"><a href="#二分查找模板" class="headerlink" title="二分查找模板"></a>二分查找模板</h1><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">lower_bound</span><span class="params">(vector&lt;<span class="type">int</span>&gt; &amp;nums, <span class="type">int</span> target)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> left = <span class="number">0</span>, right = (<span class="type">int</span>) nums.<span class="built_in">size</span>() - <span class="number">1</span>; </span><br><span class="line">        <span class="comment">// 闭区间 [left, right]</span></span><br><span class="line">        <span class="keyword">while</span> (left &lt;= right) &#123; <span class="comment">// 区间不为空</span></span><br><span class="line">            <span class="comment">// 循环不变量：</span></span><br><span class="line">            <span class="comment">// nums[left-1] &lt; target</span></span><br><span class="line">            <span class="comment">// nums[right+1] &gt;= target</span></span><br><span class="line">            <span class="type">int</span> mid = left + (right - left) / <span class="number">2</span>;</span><br><span class="line">            <span class="keyword">if</span> (nums[mid] &lt; target) &#123;</span><br><span class="line">                left = mid + <span class="number">1</span>; <span class="comment">// 范围缩小到 [mid+1, right]</span></span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                right = mid - <span class="number">1</span>; <span class="comment">// 范围缩小到 [left, mid-1]</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> left;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><h1 id="常用的二分查找C-函数"><a href="#常用的二分查找C-函数" class="headerlink" title="常用的二分查找C++函数"></a>常用的二分查找C++函数</h1><ul><li>lower_bound<br>返回指向范围中<span class='p red'>第一个不小于</span>(<emp>即大于或等于</emp>)给定值的元素的迭代器</li><li>upper_bound<br>返回指向范围中<span class='p red'>第一个大于</span>给定值的元素的迭代器<h2 id="一些用法举例"><a href="#一些用法举例" class="headerlink" title="一些用法举例"></a>一些用法举例</h2></li><li><p>直接确定查找：</p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">auto</span> low = std::<span class="built_in">lower_bound</span>(v.<span class="built_in">begin</span>(), v.<span class="built_in">end</span>(), <span class="number">4</span>); <span class="comment">// 找到第一个不小于 4 的位置</span></span><br></pre></td></tr></table></figure><p>位置就是：low - v.begin()<br>迭代器指向的元素用：*it去取</p></li><li><p>循环查找：</p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; nums.<span class="built_in">size</span>(); ++j) </span><br><span class="line">    <span class="keyword">auto</span> r = <span class="built_in">upper_bound</span>(nums.<span class="built_in">begin</span>(), nums.<span class="built_in">begin</span>() + j, upper - nums[j]); </span><br><span class="line">    <span class="comment">// &lt;= upper-nums[j] 的 nums[i] 的个数</span></span><br><span class="line">    <span class="keyword">auto</span> l = <span class="built_in">lower_bound</span>(nums.<span class="built_in">begin</span>(), nums.<span class="built_in">begin</span>() + j, lower - nums[j]); </span><br><span class="line">    <span class="comment">// &lt; lower-nums[j] 的 nums[i] 的个数</span></span><br></pre></td></tr></table></figure></li></ul>]]></content>
      
      
      <categories>
          
          <category> LeetCode刷题总结 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LeetCode </tag>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习基础语法</title>
      <link href="/2024/05/14/pytorch/"/>
      <url>/2024/05/14/pytorch/</url>
      
        <content type="html"><![CDATA[<h1 id="argmax用法"><a href="#argmax用法" class="headerlink" title="argmax用法"></a>argmax用法</h1><p> 返回的是下标<br> 关于dim的详细用法见 <a href="https://blog.csdn.net/m0_37637704/article/details/105154785?spm=1001.2101.3001.6650.4&amp;utm_medium=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~Rate-4-105154785-blog-122368033.pc_relevant_multi_platform_whitelistv3&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~Rate-4-105154785-blog-122368033.pc_relevant_multi_platform_whitelistv3&amp;utm_relevant_index=7">argmax—-dim</a></p><h1 id="广播机制"><a href="#广播机制" class="headerlink" title="广播机制"></a>广播机制</h1><p> 广播机制详见 <a href="https://luogantt.blog.csdn.net/article/details/117925855">张量的广播机制</a></p><h1 id="torch-matmul"><a href="#torch-matmul" class="headerlink" title="torch.matmul()"></a>torch.matmul()</h1><p>矩阵乘法（第一个参量一般是batch，后面的按照矩阵乘法来计算，可能会有广播机制）<br> <a href="https://blog.csdn.net/qsmx666/article/details/105783610/">matmul</a></p><h1 id="torch-scatter"><a href="#torch-scatter" class="headerlink" title="torch.scatter_"></a>torch.scatter_</h1><p>scatter （分散）一般用于生成one-hot向量 <a href="https://blog.csdn.net/qq_39004117/article/details/95665418">scatter_</a><br>scatter_(input, dim, index, src) <strong>将src中数据根据index中的索引按照dim的方向填进input中</strong>。<br><span class='p red'>dim = 0 按列填充</span></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">src = torch.arange(<span class="number">1</span>, <span class="number">11</span>).reshape((<span class="number">2</span>, <span class="number">5</span>))</span><br><span class="line"><span class="built_in">print</span>(src)</span><br><span class="line">index = torch.tensor([[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>]])</span><br><span class="line">opt=torch.zeros(<span class="number">3</span>, <span class="number">5</span>, dtype=src.dtype).scatter_(<span class="number">0</span>, index, src)</span><br><span class="line"><span class="built_in">print</span>(opt)</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tensor([[ <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>,  <span class="number">4</span>,  <span class="number">5</span>],</span><br><span class="line">        [ <span class="number">6</span>,  <span class="number">7</span>,  <span class="number">8</span>,  <span class="number">9</span>, <span class="number">10</span>]])</span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">4</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">0</span>, <span class="number">0</span>]])</span><br></pre></td></tr></table></figure><span class='p red'>dim = 1 按行填充</span><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">src = torch.arange(<span class="number">1</span>, <span class="number">11</span>).reshape((<span class="number">2</span>, <span class="number">5</span>))</span><br><span class="line"><span class="built_in">print</span>(src)</span><br><span class="line">index = torch.tensor([[<span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">4</span>]])</span><br><span class="line">opt=torch.zeros(<span class="number">3</span>, <span class="number">5</span>, dtype=src.dtype).scatter_(<span class="number">1</span>, index, src)</span><br><span class="line"><span class="built_in">print</span>(opt)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tensor([[ <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>,  <span class="number">4</span>,  <span class="number">5</span>],</span><br><span class="line">        [ <span class="number">6</span>,  <span class="number">7</span>,  <span class="number">8</span>,  <span class="number">9</span>, <span class="number">10</span>]])</span><br><span class="line">tensor([[<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">4</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]])</span><br></pre></td></tr></table></figure><h1 id="torch-gather"><a href="#torch-gather" class="headerlink" title="torch.gather"></a>torch.gather</h1><p>这个博文将的非常的清楚  <a href="https://zhuanlan.zhihu.com/p/352877584">torch.gather</a><br> <code>.t()操作相当于矩阵的转置，列 / 行 向量相互转换</code><br><strong>gather相当于【根据坐标，去t上找值】。<br>scatter_相当于【根据坐标，把x值填入前置对象中】</strong><br>使用的时候都是embedding.scatter_</p><h1 id="nn-Embeddings"><a href="#nn-Embeddings" class="headerlink" title="nn.Embeddings"></a>nn.Embeddings</h1><p> <a href="https://blog.csdn.net/qq_39540454/article/details/115215056">nn.Embeddings</a><br> 是一个lookup table，存储了固定大小的dictionary（的word embeddings）。输入是indices，来获取指定indices的word embedding向量。<br><strong>a=embedding(input)是去embedding.weight中取对应index的词向量</strong></p><h1 id="uniform-函数"><a href="#uniform-函数" class="headerlink" title="uniform_函数"></a>uniform_函数</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line">random.uniform(x, y)</span><br></pre></td></tr></table></figure><p>uniform()  方法将随机生成下一个实数，它在[x,y]范围内。</p><h1 id="torch-arange-view"><a href="#torch-arange-view" class="headerlink" title="torch.arange.view()!"></a>torch.arange.view()!</h1><span class='p red'>!!!一定要学会创建张量方式</span><h1 id="torch-sum"><a href="#torch-sum" class="headerlink" title="torch.sum"></a>torch.sum</h1><p>keepdim:求和之后这个dim的元素个数为１，所以要被去掉，如果要保留这个维度，则应当keepdim=True</p><ul><li><span class='p red'>dim=0：将第0维相加，第1维不动</span></li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a=torch.arange(<span class="number">1</span>,<span class="number">13</span>).view(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line">b=torch.<span class="built_in">sum</span>(a**<span class="number">2</span>, dim=<span class="number">0</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(b)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tensor([[ <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>,  <span class="number">4</span>],</span><br><span class="line">        [ <span class="number">5</span>,  <span class="number">6</span>,  <span class="number">7</span>,  <span class="number">8</span>],</span><br><span class="line">        [ <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>]])</span><br><span class="line">tensor([[<span class="number">107</span>, <span class="number">140</span>, <span class="number">179</span>, <span class="number">224</span>]])</span><br></pre></td></tr></table></figure><ul><li><span class='p red'>dim=1：将第1维相加，第0维不动</span></li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">b=torch.<span class="built_in">sum</span>(a**<span class="number">2</span>, dim=<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tensor([[ <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>,  <span class="number">4</span>],</span><br><span class="line">        [ <span class="number">5</span>,  <span class="number">6</span>,  <span class="number">7</span>,  <span class="number">8</span>],</span><br><span class="line">        [ <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>]])</span><br><span class="line">tensor([[ <span class="number">30</span>],</span><br><span class="line">        [<span class="number">174</span>],</span><br><span class="line">        [<span class="number">446</span>]])</span><br></pre></td></tr></table></figure><p>如果去掉keepdim，则变成</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tensor([<span class="number">107</span>, <span class="number">140</span>, <span class="number">179</span>, <span class="number">224</span>])</span><br><span class="line">tensor([ <span class="number">30</span>, <span class="number">174</span>, <span class="number">446</span>])</span><br></pre></td></tr></table></figure><p>数据类型，在括号内直接声明dtype<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># CPU</span></span><br><span class="line">a=torch.arange(<span class="number">1</span>, <span class="number">10</span>, <span class="number">2</span>,device=<span class="string">&quot;cpu&quot;</span>,dtype=torch.float16)    <span class="comment"># 不可以</span></span><br><span class="line">a=torch.arange(<span class="number">1</span>, <span class="number">10</span>, <span class="number">2</span>,device=<span class="string">&quot;cpu&quot;</span>,dtype=torch.float32)    <span class="comment"># 可以</span></span><br><span class="line">a=torch.arange(<span class="number">1</span>, <span class="number">10</span>, <span class="number">2</span>,device=<span class="string">&quot;cpu&quot;</span>,dtype=torch.float64)    <span class="comment"># 可以</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># GPU</span></span><br><span class="line">a=torch.arange(<span class="number">1</span>, <span class="number">10</span>, <span class="number">2</span>, device=<span class="string">&quot;cuda:0&quot;</span>,dtype=torch.float16)    <span class="comment"># 可以</span></span><br><span class="line">a=torch.arange(<span class="number">1</span>, <span class="number">10</span>, <span class="number">2</span>, device=<span class="string">&quot;cuda:0&quot;</span>,dtype=torch.float32)    <span class="comment"># 可以</span></span><br><span class="line">a=torch.arange(<span class="number">1</span>, <span class="number">10</span>, <span class="number">2</span>, device=<span class="string">&quot;cuda:0&quot;</span>,dtype=torch.float64)    <span class="comment"># 可以</span></span><br><span class="line"></span><br></pre></td></tr></table></figure></p><h1 id="nn-ConvTranspose2d"><a href="#nn-ConvTranspose2d" class="headerlink" title="nn.ConvTranspose2d"></a>nn.ConvTranspose2d</h1><p>传统的卷积通常是将大图片卷积成一张小图片，而反卷积就是反过来，将一张<strong>小图片变成大图片</strong><br>在传统卷积中，我们的 padding 范围为[0,p-1]，p = 0 被称为 No padding，p = k − 1 被称为 Full Padding。<br><img src="/img/transconv.bmp" alt="转置卷积"><br>这是一个padding=0的反卷积，因为在反卷积中，p ′= k − 1 − p 。<br>也就是当我们传 p ′ = 0 时，相当于在传统卷积中传了 p = k − 1 ，而传 p ′ = k − 1 时，相当于在传统卷积中传了 p = 0 。</p><h1 id="next-和-iter-的用法"><a href="#next-和-iter-的用法" class="headerlink" title="next() 和 iter() 的用法"></a>next() 和 iter() 的用法</h1><p>next()函数用于取出可迭代对象的元素，一般与iter()函数联合使用。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">next</span>(iterobject, defalt)</span><br></pre></td></tr></table></figure><ol><li>iterobject：可迭代的对象</li><li>default：可选</li></ol><p>当第二个参数不写入的时候，如果可迭代的元素全部取出来后，会返回Stoplteration的异常；<br>当第二个参数写入的时候，可迭代对象完之后，会一直返回第二个参数写的数值。</p><h1 id="register-buffer"><a href="#register-buffer" class="headerlink" title="register_buffer"></a>register_buffer</h1><p>在内存中定一个常量，同时，模型保存和加载的时候可以写入和读出。</p><p>pytorch一般情况下，是将网络中的参数保存成orderedDict形式的，这里的参数其实包含两种，一种是各种module含的参数，即nn.Parameter。另一种就是buffer，前者每次optim.step会得到更新，而不会更新后者。</p><ul><li>网络存储时也会将buffer存下，当网络load模型时，会将存储的模型的buffer也进行赋值。</li><li>buffer的更新在forward中，optim.step只能更新nn.parameter类型的参数。</li></ul><h1 id="变量-变量"><a href="#变量-变量" class="headerlink" title="变量@变量"></a>变量@变量</h1><p>出现在一行代码中间位置的@这个@是矩阵相乘的意思，</p><p>就是比如</p><ul><li>A的shape是[batch_size, sequence_length2, sequence_length1]</li><li>B的shape是[batch_size, sequence_length1, hidden_dim]</li><li>C = A @ B, 则C的shape是[batch_size, sequence_length2, hidden_dim]</li></ul><h1 id="sys-argv-1"><a href="#sys-argv-1" class="headerlink" title="sys.argv[1]"></a>sys.argv[1]</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">python temp.py a b c d</span><br></pre></td></tr></table></figure><p>python temp.py 时，同时传入4个参数：a、b、c、d </p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sys.argv == [<span class="string">&quot;temp.py&quot;</span>,<span class="string">&quot;a&quot;</span>,<span class="string">&quot;b&quot;</span>,<span class="string">&quot;c&quot;</span>,<span class="string">&quot;d&quot;</span>]  <span class="comment"># sys.argv是持有5个元素的list对象</span></span><br><span class="line"></span><br><span class="line">sys.argv[<span class="number">0</span>]  == <span class="string">&quot;temp.py&quot;</span>   <span class="comment">#第1个元素为模块名“temp.py”</span></span><br><span class="line"></span><br><span class="line">sys.argv[<span class="number">1</span>] == <span class="string">&quot;a&quot;</span>               <span class="comment">#第2个元素为&quot;a&quot;</span></span><br><span class="line"></span><br><span class="line">sys.argv[<span class="number">2</span>] == <span class="string">&quot;b&quot;</span>               <span class="comment">#第3个元素为&quot;b&quot;</span></span><br><span class="line"></span><br><span class="line">sys.argv[<span class="number">3</span>] == <span class="string">&quot;c&quot;</span>               <span class="comment">#第4个元素为&quot;c&quot;</span></span><br><span class="line"></span><br><span class="line">sys.argv[<span class="number">4</span>] == <span class="string">&quot;d&quot;</span>               <span class="comment">#第5个元素为&quot;d&quot;</span></span><br></pre></td></tr></table></figure><h1 id="torch-clamp"><a href="#torch-clamp" class="headerlink" title="torch.clamp"></a>torch.clamp</h1><p>torch.clamp(input, min, max, out=None) → Tensor<br>将输入input张量每个元素的夹紧到区间 [min,max] 并返回一个新张量<br><figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">      | min, <span class="keyword">if</span> x_i &lt; min</span><br><span class="line">y_i = | x_i, <span class="keyword">if</span> min &lt;= x_i &lt;= max</span><br><span class="line">      | max, <span class="keyword">if</span> x_i &gt; max</span><br></pre></td></tr></table></figure></p><h1 id="nn-PixelShuffle"><a href="#nn-PixelShuffle" class="headerlink" title="nn.PixelShuffle"></a>nn.PixelShuffle</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">torch</span>.nn.PixleShuffle(upscale_factor)</span><br></pre></td></tr></table></figure><p>这里的upscale_factor就是放大的倍数，数据类型为int。</p><ul><li>输入：(N, C x upscale_factor ^2, H, W)</li><li>输出：(N, C, H x upscale_factor, W x upscale_factor)</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>pixelshuffle = nn.PixelShuffle(<span class="number">3</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">input</span> = torch.tensor(<span class="number">1</span>, <span class="number">9</span>, <span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output = pixelshuffle(<span class="built_in">input</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(output.size())</span><br><span class="line">torch.Size([<span class="number">1</span>, <span class="number">1</span>, <span class="number">12</span>, <span class="number">12</span>])</span><br></pre></td></tr></table></figure><h1 id="torch-utils-data-Dataset"><a href="#torch-utils-data-Dataset" class="headerlink" title="torch.utils.data.Dataset"></a>torch.utils.data.Dataset</h1><p>自己定义的 dataset 类需要继承 Dataset。<br>需要实现必要的魔法方法:<br>在 <code>__init__</code>方法里面进行 读取数据文件 。<br>在 <code>__getitem__</code>方法里支持通过下标访问数据。<br>在<code>__len__</code>方法里返回自定义数据集的大小，方便后期遍历。<br><a href="https://blog.csdn.net/weixin_44211968/article/details/123744513">torch.utils.data.Dataset</a></p><h1 id="torch-utils-data-Dataloader"><a href="#torch-utils-data-Dataloader" class="headerlink" title="torch.utils.data.Dataloader"></a>torch.utils.data.Dataloader</h1><p><a href="https://blog.csdn.net/weixin_44211968/article/details/123739994">torch.utils.data.Dataloader</a></p><h1 id="torch-einsum"><a href="#torch-einsum" class="headerlink" title="torch.einsum"></a>torch.einsum</h1><p>用 i 和 j 去表示矩阵的行和列，比如矩阵乘法的实现</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> torch</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>A = torch.randn(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>B = torch.randn(<span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>C = torch.einsum(<span class="string">&#x27;ik,kj-&gt;ij&#x27;</span>, A, B)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>C.shape</span><br><span class="line">torch.Size([<span class="number">3</span>, <span class="number">5</span>])</span><br></pre></td></tr></table></figure><p><a href="https://zhuanlan.zhihu.com/p/434232512">torch.einsum</a></p><h1 id="np-random-choice"><a href="#np-random-choice" class="headerlink" title="np.random.choice()"></a>np.random.choice()</h1><p><a href="https://blog.csdn.net/weixin_45459911/article/details/105955267">np.random.choice</a></p><h1 id="F-one-hot"><a href="#F-one-hot" class="headerlink" title="F.one_hot()"></a>F.one_hot()</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.arange(<span class="number">0</span>,<span class="number">5</span>) % <span class="number">3</span></span><br></pre></td></tr></table></figure><p>即 <strong><em>tensor([0, 1, 2, 0, 1])</em></strong></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">F.one_hot(torch.arange(<span class="number">0</span>, <span class="number">5</span>) % <span class="number">3</span>)</span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>F.one_hot(torch.arange(<span class="number">0</span>, <span class="number">5</span>) % <span class="number">3</span>, num_classes=<span class="number">5</span>)</span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]])</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 基础语法 </tag>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>服务器常用指令</title>
      <link href="/2024/05/07/common/"/>
      <url>/2024/05/07/common/</url>
      
        <content type="html"><![CDATA[<h2 id="服务器使用"><a href="#服务器使用" class="headerlink" title="服务器使用"></a>服务器使用</h2><div class="tabs" id="11"><ul class="nav-tabs"><button type="button" class="tab  active" data-href="11-1">docker相关</button><button type="button" class="tab " data-href="11-2">挂载映射</button><button type="button" class="tab " data-href="11-3">pip install 换源</button></ul><div class="tab-contents"><div class="tab-item-content active" id="11-1"><p><strong>创建docker</strong><br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">NV_GPU=0,1,2,3 nvidia-docker run --shm-size=4g --<span class="built_in">rm</span> -d -it --net=host --cap-add sys_admin --privileged --name kyz_ks yun.nju.edu.cn:5000/lm/pytorch:1.8.1-cuda11.1-cudnn8-devel bash</span><br></pre></td></tr></table></figure><br>如果是干净的容器(直接从dockerhub上拉下来的)，需要执行下面的指令安装nfs方便下一步挂载存储服务器：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">apt update</span><br><span class="line">apt install nfs-common -y</span><br><span class="line">apt install nfs-kernel-server -y</span><br></pre></td></tr></table></figure><br><strong>docker打包</strong><br>但是慎用这个打包，之前发现打包完有60多个G，还没搞清楚为什么这么大……Σ( ° △ °|||)︴<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker commit kyz_scc yun.nju.edu.cn:5000/kyz/pytorch:1.8.0-cuda11.1-cudnn8-devel</span><br></pre></td></tr></table></figure><br><strong>docker上传成为镜像</strong><br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker push yun.nju.edu.cn:5000/kyz/pytorch:1.8.0-cuda11.1-cudnn8-devel</span><br></pre></td></tr></table></figure><br><strong>删除容器</strong><br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker rmi -f (<span class="built_in">id</span>号)</span><br></pre></td></tr></table></figure><br><strong>删除镜像</strong><br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker <span class="built_in">rm</span> -f (<span class="built_in">id</span>号)</span><br></pre></td></tr></table></figure></p></div><div class="tab-item-content" id="11-2"><p><strong>代码</strong>挂载：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mount -t nfs -o rw,nolock,vers=3 192.168.1.145:/sf/kyz/kyz_prjs/ ./workspace/kyz</span><br></pre></td></tr></table></figure><br><strong>数据集</strong>挂载：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mount -t nfs -o rw,nolock,vers=3 192.168.1.145:/sf/shared/flicker/ ./workspace/kyz/dataset/flick2k_ori</span><br><span class="line">mount -t nfs -o rw,nolock,vers=3 192.168.1.145:/sf/shared/ ./workspace/kyz/shared</span><br></pre></td></tr></table></figure><br><strong>取消</strong>挂载：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">umount 192.168.1.145:/sf/kyz/kyz_prjs/ ./workspace/kyz</span><br></pre></td></tr></table></figure></p></div><div class="tab-item-content" id="11-3"><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">pip install xxx -i https://pypi.tuna.tsinghua.edu.cn/simple</span><br></pre></td></tr></table></figure></div></div><div class="tab-to-top"><button type="button" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div><h2 id="常用的检查指令"><a href="#常用的检查指令" class="headerlink" title="常用的检查指令"></a>常用的检查指令</h2><div class="tabs" id="111"><ul class="nav-tabs"><button type="button" class="tab  active" data-href="111-1">检查cuda是否可用</button><button type="button" class="tab " data-href="111-2">虚拟环境</button><button type="button" class="tab " data-href="111-3">实用小指令</button></ul><div class="tab-contents"><div class="tab-item-content active" id="111-1"><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">import torch </span><br><span class="line"><span class="built_in">print</span>(torch.cuda.is_available())</span><br></pre></td></tr></table></figure></div><div class="tab-item-content" id="111-2"><p>进入虚拟环境：source myenv/bin/activate<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">source</span> /opt/conda/bin/activate base</span><br></pre></td></tr></table></figure><br>退出虚拟环境：deactivate<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">source</span> /opt/conda/bin/deactivate xxx(name)</span><br></pre></td></tr></table></figure><br>删除虚拟环境：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">conda remove -n xxx（name） --all</span><br></pre></td></tr></table></figure></p></div><div class="tab-item-content" id="111-3"><p><strong>输出重定向</strong>：将程序的输出（通常是打印到控制台的）发送到其他位置，比如文件或其他设备。<br>加一个’&gt;’和保存文件名就好啦~<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">python compute_flops.py &gt; out.txt</span><br></pre></td></tr></table></figure><br><strong>保存图片</strong><br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torchvision.utils <span class="keyword">import</span> make_grid</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line">img_np = make_grid(img, nrow=<span class="built_in">int</span>(math.sqrt(img.size(<span class="number">0</span>))), normalize=<span class="literal">False</span>).cpu().numpy()</span><br><span class="line">img_np = img_np.transpose(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>)</span><br><span class="line">img_np = (img_np * <span class="number">255.0</span>).<span class="built_in">round</span>()</span><br><span class="line">img_np = img_np.astype(np.uint8)</span><br><span class="line">img_save = Image.fromarray(img_np, <span class="string">&quot;RGB&quot;</span>)</span><br><span class="line">img_save.save(<span class="string">f&quot;img.png&quot;</span>)</span><br></pre></td></tr></table></figure></p><p><strong>查看文件夹下有多少文件（包含子目录）</strong><br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">ls</span> -lR| grep <span class="string">&quot;^-&quot;</span> | <span class="built_in">wc</span> -l</span><br></pre></td></tr></table></figure></p><p><strong>复制</strong><br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cp</span> -r /workspace/kyz/div2k/* /workspace/kyz/all/clic</span><br></pre></td></tr></table></figure></p><p><strong>移动</strong><br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">mv</span> /kyz_prjs/* ./ </span><br></pre></td></tr></table></figure></p></div></div><div class="tab-to-top"><button type="button" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 服务器 </tag>
            
            <tag> 基础语法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2024/04/24/hello-world/"/>
      <url>/2024/04/24/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
